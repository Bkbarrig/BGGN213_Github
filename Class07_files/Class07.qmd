---
title: "Class 7: Machine Learning 1"
author: "Bianca Barriga"
format: pdf
---
In this class we will explor and get practice with clustering and Principal Componenet Analysis (PCA). 

#Clustering with K-means

First we will make up some data to ncluster where we know what the result should be. 

```{r}
hist(rnorm(3000000, mean =-3))
```
I want a little vector with two groupings in it:
```{r}
rnorm(30, -3)
rnorm(30, +3)
```

```{r}
tmp <- c(rnorm(30,-3), rnorm(30,+3))
x <- data.frame(x=tmp, y=rev(tmp))
head(x)
```
Lets have a look: 
```{r}
plot(x)
```

Have a look at help for kmeans()
```{r}
#input data = x, centers = the number of clusters
km <- kmeans(x, centers= 2 )
km
#cluster means is where the centroid of the clusters are 
```

It is important to not just run the analysis but to be able to get your important results back in a way that we can do things with them. 

>Q. How do I find the cluster size 

```{r}
km$size
```
>Q. How about the cluster centers?

```{r}
km$centers
```
>Q. How about the main result - the cluster assignment?

```{r}
km$cluster
```
>Q. Can we make a summary figure showing the result? 
- The points colored by cluster assignment and add the cluster centers as a different color?

```{r}
plot(x, col=c("red", "blue"))
```


```{r}
plot(x, col=c(1,1,1,1,1,1,1,2))

```

```{r}
plot(x, col=km$cluster)
```

Lets plot the data with ggplot. 
I need 3 things: 
librar
```{r}
library(ggplot2)
ggplot(x) + 
  aes(x,y) +
  geom_point(col=km$cluster)
```
```{r}
#Make up a color vector
mycols <- rep("gray", 60)
mycols
```
```{r}
plot(x, col=mycols)
```

Let's highlight points 10, 12, and 20. 
```{r}
mycols[c(10, 12, 20)] <- "red"

plot(x, col=mycols, pch = 18)

```
Play with different numbers of centers

```{r}
km <- kmeans(x,centers=2)
plot(x, col=km$cluster)
```
```{r}
km$tot.withinss
#can use to generate a SCREE plot 
```
What we want to do is try out different numbers of K from 1 to 7. We can write a `for` loop to do this for us to store the `tot.withinss` each time. 

```{r}
#create empty vector
totss <- NULL
#designate k
k <- 1:7

for(i in k) {
  #cat(i, "\n")
 totss <-  c(totss,kmeans(x, centers =i)$tot.withinss)
}
```

```{r}
plot(totss, typ='o')
```

#Hierarchical Clustering 

We can't just give the `hclust()` function teh data `x` like we did for `kmeans()`. Each cluster is a lot more flexible so we can give it identities. 

We need to first calculate a "distance matrix". The `dist()` function by default will calculate euclidean distance. 


```{r}
d <- dist(x)
hc <- hclust(d)
hc
```
The print out from `hclust` is not very helpful, but the plot method is very useful. 
Height is important for relatedness. 
```{r}
plot(hc)
abline(h=10, col="red", lty=2)
```
To get my all important cluster membership vector out of a
hclust ovject I can use the `cutree()`


```{r}
cutree(hc, h=10)
```
You can also set a `k=` argument for `cutree()`. 

```{r}
grps <- cutree(hc,k=2)
```
Let's make a figure of our hclust results. 

```{r}
plot(x, col=grps)
```
#Principal Component Analysis (PCA) 
PCA projects the features onto a principal componenet. The motivation is to reduce the features dimensionality while only losing a small amount of information. Principal components are new low dimensional axis (or surfaces) closest to the observations. 

PCA objectives
-reduce dimensionality
-visualize multidimensional data
-choose the most useful variables
-identify groups of objects (e.g. genes.samples)
-identify outliers

The main base R function to do PCA is called `prcomp()`.  

First I need to import the data. 
```{r}
url <- "http://tinyurl.com/UK-foods"
x <- read.csv(url)
head(x)
```
The row names are showing up as data points. We need to change this to row names and not the first column. 

```{r}
rownames(x) <-x[,1]
x <- x[,-1]
head(x)

```
A better way to do this is, because if you run the code above multiple times it will continue to replace the row names. We can correct the row names when we upload the file. 
```{r}
x <-read.csv(url, row.names = 1)
head(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

>Q3. Changing what optional argument in the above `barplot()` function reuslts in the following plot? 

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

>Q5. Generating all pairwise plots may help somewhat can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a give plot?

If the given point lies on the diagonal, it means that the points they are the same or very similar.  


```{r}
pairs(x, col=rainbow(10), pch=16)
```

>Q6. What are the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

The `prcomp()` function expects the observations to be rows and the variables to be columns therefore we need to first transpose our `data.frame` matrix with the `t()` transpose function. 

```{r}
# Use the prcomp() PCA function 
pca <- prcomp( t(x) )
summary(pca)
```
```{r}
attributes(pca)
```

```{r}
pca$x
```

Make my PC1 vs PC2 plot, "score plot", "PCA plot", etc.

```{r}
plot(pca$x[,1], pca$x[,2])
```
```{r}
plot(pca$x[,1], pca$x[,2])
```


Color the points by country. 

```{r}
mycols = c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16, xlab = "PC1", ylab="PC2")

```

```{r}
loadings <- as.data.frame(pca$rotation)
ggplot(loadings) +
  aes(PC1, rownames(loadings)) +
  geom_col()
```





